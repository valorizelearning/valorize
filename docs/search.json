[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VALORIZE",
    "section": "",
    "text": "Meliana Christianti Johan | NIM 33224001\nPromotor: Prof. Ir. Armein Z. R. Langi, M.Sc., Ph.D.\nCo-Promotor: Ir. I Gusti Bagus Baskara Nugraha, S.T., M.T., Ph.D.\nJudul Disertasi:\nKerangka Kerja Manajemen Pengetahuan Kolaboratif Berbasis Rekayasa Cerdas untuk Pendidikan Tinggi Berorientasi Nilai\nCollaborative Knowledge Management Framework Based on Smart Engineering for Value-Oriented Higher Education\n\nIntroduction\nValorize Learning Framework Klik Disini\n\n\n\n\n\n\nKata “Valorize” berasal dari bahasa Prancis valoriser dan akar Latin valor (nilai). Dalam konteks pendidikan, Valorize berarti mengakui, mengembangkan, dan merealisasikan potensi yang ada menjadi nilai nyata yang dapat diukur.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "valorize_introduction.html",
    "href": "valorize_introduction.html",
    "title": "1  Introduction to Valorize",
    "section": "",
    "text": "1.1 VALORIZE LEARNING: Where Value is Realized!\nValorize\nKerangka pembelajaran transformatif yang mengintegrasikan Value (nilai), Collaboration (kolaborasi), Artificial Intelligence (Kecerdasan Buatan), dan Personalized Learning (Pembelajaran yang dipersonalisasi) untuk menciptakan pembelajaran yang bermakna (Value Co-Creation) dalam konteks pendidikan berorientasi nilai (Value Oriented Education).",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Valorize</span>"
    ]
  },
  {
    "objectID": "valorize_introduction.html#visi-misi",
    "href": "valorize_introduction.html#visi-misi",
    "title": "1  Introduction to Valorize",
    "section": "1.2 Visi Misi",
    "text": "1.2 Visi Misi\n\n1.2.1 Visi\nMenjadi kerangka kerja pembelajaran transformatif yang mengakui, mengembangkan, dan merealisasikan potensi peserta didik menjadi kompetensi profesional yang autentik.\n\n\n1.2.2 Misi\nMemfasilitasi transformasi peserta didik dari konsumen pengetahuan pasif menjadi produsen nilai aktif melalui: Kolaborasi bermakna dalam Knowledge Marketplace berbasis peer production Pembelajaran berbasis proyek autentik yang relevan dengan kebutuhan profesional Pengembangan identitas profesional yang kuat dan adaptif.\nKomponen inti framework: 1. VALue (Nilai-nilai luhur sebagai fondasi) 2. Organized through CollaRation (Pembelajaran kolaboratif terorganisir) 3. Intelligence (Kecerdasan Buatan sebagai enabler) 4. Zones of PersonalIzed & Zenith Education (Zona pembelajaran personal menuju puncak potensi)",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Valorize</span>"
    ]
  },
  {
    "objectID": "altair_marks_encoding.html",
    "href": "altair_marks_encoding.html",
    "title": "2  Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "",
    "text": "2.1 Global Development Data\nA visualization represents data using a collection of graphical marks (bars, lines, points, etc.). The attributes of a mark — such as its position, shape, size, or color — serve as channels through which we can encode underlying data values.\nWith a basic framework of data types, marks, and encoding channels, we can concisely create a wide variety of visualizations. In this notebook, we explore each of these elements and show how to use them to create custom statistical graphics.\nThis notebook is part of the data visualization curriculum.\nWe will be visualizing global health and population data for a number of countries, over the time period of 1955 to 2005. The data was collected by the Gapminder Foundation and shared in Hans Rosling’s popular TED talk. If you haven’t seen the talk, we encourage you to watch it first!\nLet’s first load the dataset from the vega-datasets collection into a Pandas data frame.\nfrom vega_datasets import data as vega_data\ndata = vega_data.gapminder()\nHow big is the data?\ndata.shape\n\n(693, 6)\n693 rows and 6 columns! Let’s take a peek at the data content:\ndata.head(5)\n\n\n\n\n\n\n\n\nyear\ncountry\ncluster\npop\nlife_expect\nfertility\n\n\n\n\n0\n1955\nAfghanistan\n0\n8891209\n30.332\n7.7\n\n\n1\n1960\nAfghanistan\n0\n9829450\n31.997\n7.7\n\n\n2\n1965\nAfghanistan\n0\n10997885\n34.020\n7.7\n\n\n3\n1970\nAfghanistan\n0\n12430623\n36.088\n7.7\n\n\n4\n1975\nAfghanistan\n0\n14132019\n38.438\n7.7\nFor each country and year (in 5-year intervals), we have measures of fertility in terms of the number of children per woman (fertility), life expectancy in years (life_expect), and total population (pop).\nWe also see a cluster field with an integer code. What might this represent? We’ll try and solve this mystery as we visualize the data!\nLet’s also create a smaller data frame, filtered down to values for the year 2000 only:\ndata2000 = data.loc[data['year'] == 2000]\ndata2000.head(5)\n\n\n\n\n\n\n\n\nyear\ncountry\ncluster\npop\nlife_expect\nfertility\n\n\n\n\n9\n2000\nAfghanistan\n0\n23898198\n42.129\n7.4792\n\n\n20\n2000\nArgentina\n3\n37497728\n74.340\n2.3500\n\n\n31\n2000\nAruba\n3\n69539\n73.451\n2.1240\n\n\n42\n2000\nAustralia\n4\n19164620\n80.370\n1.7560\n\n\n53\n2000\nAustria\n1\n8113413\n78.980\n1.3820",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types, Graphical Marks, and Visual Encoding Channels</span>"
    ]
  },
  {
    "objectID": "altair_marks_encoding.html#data-types",
    "href": "altair_marks_encoding.html#data-types",
    "title": "2  Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "2.2 Data Types",
    "text": "2.2 Data Types\nThe first ingredient in effective visualization is the input data. Data values can represent different forms of measurement. What kinds of comparisons do those measurements support? And what kinds of visual encodings then support those comparisons?\nWe will start by looking at the basic data types that Altair uses to inform visual encoding choices. These data types determine the kinds of comparisons we can make, and thereby guide our visualization design decisions.\n\n2.2.1 Nominal (N)\nNominal data (also called categorical data) consist of category names.\nWith nominal data we can compare the equality of values: is value A the same or different than value B? (A = B), supporting statements like “A is equal to B” or “A is not equal to B”. In the dataset above, the country field is nominal.\nWhen visualizing nominal data we should readily be able to see if values are the same or different: position, color hue (blue, red, green, etc.), and shape can help. However, using a size channel to encode nominal data might mislead us, suggesting rank-order or magnitude differences among values that do not exist!\n\n\n2.2.2 Ordinal (O)\nOrdinal data consist of values that have a specific ordering.\nWith ordinal data we can compare the rank-ordering of values: does value A come before or after value B? (A &lt; B), supporting statements like “A is less than B” or “A is greater than B”. In the dataset above, we can treat the year field as ordinal.\nWhen visualizing ordinal data, we should perceive a sense of rank-order. Position, size, or color value (brightness) might be appropriate, where as color hue (which is not perceptually ordered) would be less appropriate.\n\n\n2.2.3 Quantitative (Q)\nWith quantitative data we can measure numerical differences among values. There are multiple sub-types of quantitative data:\nFor interval data we can measure the distance (interval) between points: what is the distance to value A from value B? (A - B), supporting statements such as “A is 12 units away from B”.\nFor ratio data the zero-point is meaningful and so we can also measure proportions or scale factors: value A is what proportion of value B? (A / B), supporting statements such as “A is 10% of B” or “B is 7 times larger than A”.\nIn the dataset above, year is a quantitative interval field (the value of year “zero” is subjective), whereas fertility and life_expect are quantitative ratio fields (zero is meaningful for calculating proportions). Vega-Lite represents quantitative data, but does not make a distinction between interval and ratio types.\nQuantitative values can be visualized using position, size, or color value, among other channels. An axis with a zero baseline is essential for proportional comparisons of ratio values, but can be safely omitted for interval comparisons.\n\n\n2.2.4 Temporal (T)\nTemporal values measure time points or intervals. This type is a special case of quantitative values (timestamps) with rich semantics and conventions (i.e., the Gregorian calendar). The temporal type in Vega-Lite supports reasoning about time units (year, month, day, hour, etc.), and provides methods for requesting specific time intervals.\nExample temporal values include date strings such as “2019-01-04” and “Jan 04 2019”, as well as standardized date-times such as the ISO date-time format: “2019-01-04T17:50:35.643Z”.\nThere are no temporal values in our global development dataset above, as the year field is simply encoded as an integer. For more details about using temporal data in Altair, see the Times and Dates documentation.\n\n\n2.2.5 Summary\nThese data types are not mutually exclusive, but rather form a hierarchy: ordinal data support nominal (equality) comparisons, while quantitative data support ordinal (rank-order) comparisons.\nMoreover, these data types do not provide a fixed categorization. Just because a data field is represented using a number doesn’t mean we have to treat it as a quantitative type! For example, we might interpret a set of ages (10 years old, 20 years old, etc) as nominal (underage or overage), ordinal (grouped by year), or quantitative (calculate average age).\nNow let’s examine how to visually encode these data types!",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types, Graphical Marks, and Visual Encoding Channels</span>"
    ]
  },
  {
    "objectID": "altair_marks_encoding.html#encoding-channels",
    "href": "altair_marks_encoding.html#encoding-channels",
    "title": "2  Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "2.3 Encoding Channels",
    "text": "2.3 Encoding Channels\nAt the heart of Altair is the use of encodings that bind data fields (with a given data type) to available encoding channels of a chosen mark type. In this notebook we’ll examine the following encoding channels:\n\nx: Horizontal (x-axis) position of the mark.\ny: Vertical (y-axis) position of the mark.\nsize: Size of the mark. May correspond to area or length, depending on the mark type.\ncolor: Mark color, specified as a legal CSS color.\nopacity: Mark opacity, ranging from 0 (fully transparent) to 1 (fully opaque).\nshape: Plotting symbol shape for point marks.\ntooltip: Tooltip text to display upon mouse hover over the mark.\norder: Mark ordering, determines line/area point order and drawing order.\ncolumn: Facet the data into horizontally-aligned subplots.\nrow: Facet the data into vertically-aligned subplots.\n\nFor a complete list of available channels, see the Altair encoding documentation.\n\n2.3.1 X\nThe x encoding channel sets a mark’s horizontal position (x-coordinate). In addition, default choices of axis and title are made automatically. In the chart below, the choice of a quantitative data type results in a continuous linear axis scale:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q')\n)\n\n\n\n\n\n\n\n\n2.3.2 Y\nThe y encoding channel sets a mark’s vertical position (y-coordinate). Here we’ve added the cluster field using an ordinal (O) data type. The result is a discrete axis that includes a sized band, with a default step size, for each unique value:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:O')\n)\n\n\n\n\n\n\nWhat happens to the chart above if you swap the O and Q field types?\nIf we instead add the life_expect field as a quantitative (Q) variable, the result is a scatter plot with linear scales for both axes:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q')\n)\n\n\n\n\n\n\nBy default, axes for linear quantitative scales include zero to ensure a proper baseline for comparing ratio-valued data. In some cases, however, a zero baseline may be meaningless or you may want to focus on interval comparisons. To disable automatic inclusion of zero, configure the scale mapping using the encoding scale attribute:\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q', scale=alt.Scale(zero=False)),\n    alt.Y('life_expect:Q', scale=alt.Scale(zero=False))\n)\n\n\n\n\n\n\nNow the axis scales no longer include zero by default. Some padding still remains, as the axis domain end points are automatically snapped to nice numbers like multiples of 5 or 10.\nWhat happens if you also add nice=False to the scale attribute above?\n\n\n2.3.3 Size\nThe size encoding channel sets a mark’s size or extent. The meaning of the channel can vary based on the mark type. For point marks, the size channel maps to the pixel area of the plotting symbol, such that the diameter of the point matches the square root of the size value.\nLet’s augment our scatter plot by encoding population (pop) on the size channel. As a result, the chart now also includes a legend for interpreting the size values.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q')\n)\n\n\n\n\n\n\nIn some cases we might be unsatisfied with the default size range. To provide a customized span of sizes, set the range parameter of the scale attribute to an array indicating the smallest and largest sizes. Here we update the size encoding to range from 0 pixels (for zero values) to 1,000 pixels (for the maximum value in the scale domain):\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]))\n)\n\n\n\n\n\n\n\n\n2.3.4 Color and Opacity\nThe color encoding channel sets a mark’s color. The style of color encoding is highly dependent on the data type: nominal data will default to a multi-hued qualitative color scheme, whereas ordinal and quantitative data will use perceptually ordered color gradients.\nHere, we encode the cluster field using the color channel and a nominal (N) data type, resulting in a distinct hue for each cluster value. Can you start to guess what the cluster field might indicate?\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nIf we prefer filled shapes, we can can pass a filled=True parameter to the mark_point method:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N')\n)\n\n\n\n\n\n\nBy default, Altair uses a bit of transparency to help combat over-plotting. We are free to further adjust the opacity, either by passing a default value to the mark_* method, or using a dedicated encoding channel.\nHere we demonstrate how to provide a constant value to an encoding channel instead of binding a data field:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5)\n)\n\n\n\n\n\n\n\n\n2.3.5 Shape\nThe shape encoding channel sets the geometric shape used by point marks. Unlike the other channels we have seen so far, the shape channel can not be used by other mark types. The shape encoding channel should only be used with nominal data, as perceptual rank-order and magnitude comparisons are not supported.\nLet’s encode the cluster field using shape as well as color. Using multiple channels for the same underlying data field is known as a redundant encoding. The resulting chart combines both color and shape information into a single symbol legend:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.3.6 Tooltips & Ordering\nBy this point, you might feel a bit frustrated: we’ve built up a chart, but we still don’t know what countries the visualized points correspond to! Let’s add interactive tooltips to enable exploration.\nThe tooltip encoding channel determines tooltip text to show when a user moves the mouse cursor over a mark. Let’s add a tooltip encoding for the country field, then investigate which countries are being represented.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country')\n)\n\n\n\n\n\n\nAs you mouse around you may notice that you can not select some of the points. For example, the largest dark blue circle corresponds to India, which is drawn on top of a country with a smaller population, preventing the mouse from hovering over that country. To fix this problem, we can use the order encoding channel.\nThe order encoding channel determines the order of data points, affecting both the order in which they are drawn and, for line and area marks, the order in which they are connected to one another.\nLet’s order the values in descending rank order by the population (pop), ensuring that smaller circles are drawn later than larger circles:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n)\n\n\n\n\n\n\nNow we can identify the smaller country being obscured by India: it’s Bangladesh!\nWe can also now figure out what the cluster field represents. Mouse over the various colored points to formulate your own explanation.\nAt this point we’ve added tooltips that show only a single property of the underlying data record. To show multiple values, we can provide the tooltip channel an array of encodings, one for each field we want to include:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Order('pop:Q', sort='descending'),\n    tooltip = [\n        alt.Tooltip('country:N'),\n        alt.Tooltip('fertility:Q'),\n        alt.Tooltip('life_expect:Q')\n    ]   \n)\n\n\n\n\n\n\nNow we can see multiple data fields upon mouse over!\n\n\n2.3.7 Column and Row Facets\nSpatial position is one of the most powerful and flexible channels for visual encoding, but what can we do if we already have assigned fields to the x and y channels? One valuable technique is to create a trellis plot, consisting of sub-plots that show a subset of the data. A trellis plot is one example of the more general technique of presenting data using small multiples of views.\nThe column and row encoding channels generate either a horizontal (columns) or vertical (rows) set of sub-plots, in which the data is partitioned according to the provided data field.\nHere is a trellis plot that divides the data into one column per `cluster` value:\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000])),\n    alt.Color('cluster:N'),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n)\n\n\n\n\n\n\nThe plot above does not fit on screen, making it difficult to compare all the sub-plots to each other! We can set the default width and height properties to create a smaller set of multiples. Also, as the column headers already label the cluster values, let’s remove our color legend by setting it to None. To make better use of space we can also orient our size legend to the 'bottom' of the chart.\n\nalt.Chart(data2000).mark_point(filled=True).encode(\n    alt.X('fertility:Q'),\n    alt.Y('life_expect:Q'),\n    alt.Size('pop:Q', scale=alt.Scale(range=[0,1000]),\n             legend=alt.Legend(orient='bottom', titleOrient='left')),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending'),\n    alt.Column('cluster:N')\n).properties(width=135, height=135)\n\n\n\n\n\n\nUnderneath the hood, the column and row encodings are translated into a new specification that uses the facet view composition operator. We will re-visit faceting in greater depth later on!\nIn the meantime, can you rewrite the chart above to facet into rows instead of columns?\n\n\n2.3.8 A Peek Ahead: Interactive Filtering\nIn later modules, we’ll dive into interaction techniques for data exploration. Here is a sneak peak: binding a range slider to the year field to enable interactive scrubbing through each year of data. Don’t worry if the code below is a bit confusing at this point, as we will cover interaction in detail later.\nDrag the slider back and forth to see how the data values change over time!\n\nselect_year = alt.selection_single(\n    name='select', fields=['year'], init={'year': 1955},\n    bind=alt.binding_range(min=1955, max=2005, step=5)\n)\n\nalt.Chart(data).mark_point(filled=True).encode(\n    alt.X('fertility:Q', scale=alt.Scale(domain=[0,9])),\n    alt.Y('life_expect:Q', scale=alt.Scale(domain=[0,90])),\n    alt.Size('pop:Q', scale=alt.Scale(domain=[0, 1200000000], range=[0,1000])),\n    alt.Color('cluster:N', legend=None),\n    alt.OpacityValue(0.5),\n    alt.Tooltip('country:N'),\n    alt.Order('pop:Q', sort='descending')\n).add_selection(select_year).transform_filter(select_year)",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types, Graphical Marks, and Visual Encoding Channels</span>"
    ]
  },
  {
    "objectID": "altair_marks_encoding.html#graphical-marks",
    "href": "altair_marks_encoding.html#graphical-marks",
    "title": "2  Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "2.4 Graphical Marks",
    "text": "2.4 Graphical Marks\nOur exploration of encoding channels above exclusively uses point marks to visualize the data. However, the point mark type is only one of the many geometric shapes that can be used to visually represent data. Altair includes a number of built-in mark types, including:\n\nmark_area() - Filled areas defined by a top-line and a baseline.\nmark_bar() - Rectangular bars.\nmark_circle() - Scatter plot points as filled circles.\nmark_line() - Connected line segments.\nmark_point() - Scatter plot points with configurable shapes.\nmark_rect() - Filled rectangles, useful for heatmaps.\nmark_rule() - Vertical or horizontal lines spanning the axis.\nmark_square() - Scatter plot points as filled squares.\nmark_text() - Scatter plot points represented by text.\nmark_tick() - Vertical or horizontal tick marks.\n\nFor a complete list, and links to examples, see the Altair marks documentation. Next, we will step through a number of the most commonly used mark types for statistical graphics.\n\n2.4.1 Point Marks\nThe point mark type conveys specific points, as in scatter plots and dot plots. In addition to x and y encoding channels (to specify 2D point positions), point marks can use color, size, and shape encodings to convey additional data fields.\nBelow is a dot plot of fertility, with the cluster field redundantly encoded using both the y and shape channels.\n\nalt.Chart(data2000).mark_point().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\nIn addition to encoding channels, marks can be stylized by providing values to the mark_*() methods.\nFor example: point marks are drawn with stroked outlines by default, but can be specified to use filled shapes instead. Similarly, you can set a default size to set the total pixel area of the point mark.\n\nalt.Chart(data2000).mark_point(filled=True, size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.4.2 Circle Marks\nThe circle mark type is a convenient shorthand for point marks drawn as filled circles.\n\nalt.Chart(data2000).mark_circle(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.4.3 Square Marks\nThe square mark type is a convenient shorthand for point marks drawn as filled squares.\n\nalt.Chart(data2000).mark_square(size=100).encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.4.4 Tick Marks\nThe tick mark type conveys a data point using a short line segment or “tick”. These are particularly useful for comparing values along a single dimension with minimal overlap. A dot plot drawn with tick marks is sometimes referred to as a strip plot.\n\nalt.Chart(data2000).mark_tick().encode(\n    alt.X('fertility:Q'),\n    alt.Y('cluster:N'),\n    alt.Shape('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.4.5 Bar Marks\nThe `bar` mark type draws a rectangle with a position, width, and height.\nThe plot below is a simple bar chart of the population (`pop`) of each country.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('country:N'),\n    alt.Y('pop:Q')\n)\n\n\n\n\n\n\nThe bar width is set to a default size. We will discuss how to adjust the bar width later in this notebook. (A subsequent notebook will take a closer look at configuring axes, scales, and legends.)\nBars can also be stacked. Let’s change the x encoding to use the cluster field, and encode country using the color channel. We’ll also disable the legend (which would be very long with colors for all countries!) and use tooltips for the country name.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('cluster:N'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n)\n\n\n\n\n\n\nIn the chart above, the use of the color encoding channel causes Altair / Vega-Lite to automatically stack the bar marks. Otherwise, bars would be drawn on top of each other! Try adding the parameter stack=None to the y encoding channel to see what happens if we don’t apply stacking…\nThe examples above create bar charts from a zero-baseline, and the y channel only encodes the non-zero value (or height) of the bar. However, the bar mark also allows you to specify starting and ending points to convey ranges.\nThe chart below uses the x (starting point) and x2 (ending point) channels to show the range of life expectancies within each regional cluster. Below we use the min and max aggregation functions to determine the end points of the range; we will discuss aggregation in greater detail in the next notebook!\nAlternatively, you can use x and width to provide a starting point plus offset, such that x2 = x + width.\n\nalt.Chart(data2000).mark_bar().encode(\n    alt.X('min(life_expect):Q'),\n    alt.X2('max(life_expect):Q'),\n    alt.Y('cluster:N')\n)\n\n\n\n\n\n\n\n\n2.4.6 Line Marks\nThe line mark type connects plotted points with line segments, for example so that a line’s slope conveys information about the rate of change.\nLet’s plot a line chart of fertility per country over the years, using the full, unfiltered global development data frame. We’ll again hide the legend and use tooltips instead.\n\nalt.Chart(data).mark_line().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nWe can see interesting variations per country, but overall trends for lower numbers of children per family over time. Also note that we set a custom width of 400 pixels. Try changing (or removing) the widths and see what happens!\nLet’s change some of the default mark parameters to customize the plot. We can set the strokeWidth to determine the thickness of the lines and the opacity to add some transparency. By default, the line mark uses straight line segments to connect data points. In some cases we might want to smooth the lines. We can adjust the interpolation used to connect data points by setting the interpolate mark parameter. Let’s use 'monotone' interpolation to provide smooth lines that are also guaranteed not to inadvertently generate “false” minimum or maximum values as a result of the interpolation.\n\nalt.Chart(data).mark_line(\n    strokeWidth=3,\n    opacity=0.5,\n    interpolate='monotone'\n).encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width=400\n)\n\n\n\n\n\n\nThe line mark can also be used to create slope graphs, charts that highlight the change in value between two comparison points using line slopes.\nBelow let’s create a slope graph comparing the populations of each country at minimum and maximum years in our full dataset: 1955 and 2005. We first create a new Pandas data frame filtered to those years, then use Altair to create the slope graph.\nBy default, Altair places the years close together. To better space out the years along the x-axis, we can indicate the size (in pixels) of discrete steps along the width of our chart as indicated by the comment below. Try adjusting the width step value below and see how the chart changes in response.\n\ndataTime = data.loc[(data['year'] == 1955) | (data['year'] == 2005)]\n\nalt.Chart(dataTime).mark_line(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N', legend=None),\n    alt.Tooltip('country:N')\n).properties(\n    width={\"step\": 50} # adjust the step parameter\n)\n\n\n\n\n\n\n\n\n2.4.7 Area Marks\nThe area mark type combines aspects of line and bar marks: it visualizes connections (slopes) among data points, but also shows a filled region, with one edge defaulting to a zero-valued baseline.\nThe chart below is an area chart of population over time for just the United States:\n\ndataUS = data.loc[data['country'] == 'United States']\n\nalt.Chart(dataUS).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to line marks, area marks support an interpolate parameter.\n\nalt.Chart(dataUS).mark_area(interpolate='monotone').encode(\n    alt.X('year:O'),\n    alt.Y('fertility:Q')\n)\n\n\n\n\n\n\nSimilar to bar marks, area marks also support stacking. Here we create a new data frame with data for the three North American countries, then plot them using an area mark and a color encoding channel to stack by country.\n\ndataNA = data.loc[\n    (data['country'] == 'United States') |\n    (data['country'] == 'Canada') |\n    (data['country'] == 'Mexico')\n]\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nBy default, stacking is performed relative to a zero baseline. However, other stack options are available:\n\ncenter - to stack relative to a baseline in the center of the chart, creating a streamgraph visualization, and\nnormalize - to normalize the summed data at each stacking point to 100%, enabling percentage comparisons.\n\nBelow we adapt the chart by setting the y encoding stack attribute to center. What happens if you instead set it normalize?\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack='center'),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nTo disable stacking altogether, set the stack attribute to None. We can also add opacity as a default mark parameter to ensure we see the overlapping areas!\n\nalt.Chart(dataNA).mark_area(opacity=0.5).encode(\n    alt.X('year:O'),\n    alt.Y('pop:Q', stack=None),\n    alt.Color('country:N')\n)\n\n\n\n\n\n\nThe area mark type also supports data-driven baselines, with both the upper and lower series determined by data fields. As with bar marks, we can use the x and x2 (or y and y2) channels to provide end points for the area mark.\nThe chart below visualizes the range of minimum and maximum fertility, per year, for North American countries:\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.X('year:O'),\n    alt.Y('min(fertility):Q'),\n    alt.Y2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)\n\n\n\n\n\n\nWe can see a larger range of values in 1995, from just under 4 to just under 7. By 2005, both the overall fertility values and the variability have declined, centered around 2 children per familty.\nAll the area mark examples above use a vertically oriented area. However, Altair and Vega-Lite support horizontal areas as well. Let’s transpose the chart above, simply by swapping the x and y channels.\n\nalt.Chart(dataNA).mark_area().encode(\n    alt.Y('year:O'),\n    alt.X('min(fertility):Q'),\n    alt.X2('max(fertility):Q')\n).properties(\n    width={\"step\": 40}\n)",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types, Graphical Marks, and Visual Encoding Channels</span>"
    ]
  },
  {
    "objectID": "altair_marks_encoding.html#summary-1",
    "href": "altair_marks_encoding.html#summary-1",
    "title": "2  Data Types, Graphical Marks, and Visual Encoding Channels",
    "section": "2.5 Summary",
    "text": "2.5 Summary\nWe’ve completed our tour of data types, encoding channels, and graphical marks! You should now be well-equipped to further explore the space of encodings, mark types, and mark parameters. For a comprehensive reference – including features we’ve skipped over here! – see the Altair marks and encoding documentation.\nIn the next module, we will look at the use of data transformations to create charts that summarize data or visualize new derived fields. In a later module, we’ll examine how to further customize your charts by modifying scales, axes, and legends.\nInterested in learning more about visual encoding?\n\nBertin’s taxonomy of visual encodings from Sémiologie Graphique, as adapted by Mike Bostock.\n\nThe systematic study of marks, visual encodings, and backing data types was initiated by Jacques Bertin in his pioneering 1967 work Sémiologie Graphique (The Semiology of Graphics). The image above illustrates position, size, value (brightness), texture, color (hue), orientation, and shape channels, alongside Bertin’s recommendations for the data types they support.\nThe framework of data types, marks, and channels also guides automated visualization design tools, starting with Mackinlay’s APT (A Presentation Tool) in 1986 and continuing in more recent systems such as Voyager and Draco.\nThe identification of nominal, ordinal, interval, and ratio types dates at least as far back as S. S. Steven’s 1947 article On the theory of scales of measurement.",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Types, Graphical Marks, and Visual Encoding Channels</span>"
    ]
  },
  {
    "objectID": "altair_data_transformation.html",
    "href": "altair_data_transformation.html",
    "title": "3  Data Transformation",
    "section": "",
    "text": "3.1 The Movies Dataset\nIn previous notebooks we learned how to use marks and visual encodings to represent individual data records. Here we will explore methods for transforming data, including the use of aggregates to summarize multiple records. Data transformation is an integral part of visualization: choosing the variables to show and their level of detail is just as important as choosing appropriate visual encodings. After all, it doesn’t matter how well chosen your visual encodings are if you are showing the wrong information!\nAs you work through this module, we recommend that you open the Altair Data Transformations documentation in another tab. It will be a useful resource if at any point you’d like more details or want to see what other transformations are available.\nThis notebook is part of the data visualization curriculum.\nWe will be working with a table of data about motion pictures, taken from the vega-datasets collection. The data includes variables such as the film name, director, genre, release date, ratings, and gross revenues. However, be careful when working with this data: the films are from unevenly sampled years, using data combined from multiple sources. If you dig in you will find issues with missing values and even some subtle errors! Nevertheless, the data should prove interesting to explore…\nLet’s retrieve the URL for the JSON data file from the vega_datasets package, and then read the data into a Pandas data frame so that we can inspect its contents.\nmovies_url = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'\nmovies = pd.read_json(movies_url)\nHow many rows (records) and columns (fields) are in the movies dataset?\nmovies.shape\n\n(3201, 16)\nNow let’s peek at the first 5 rows of the table to get a sense of the fields and data types…\nmovies.head(5)\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation</span>"
    ]
  },
  {
    "objectID": "altair_data_transformation.html#histograms",
    "href": "altair_data_transformation.html#histograms",
    "title": "3  Data Transformation",
    "section": "3.2 Histograms",
    "text": "3.2 Histograms\nWe’ll start our transformation tour by binning data into discrete groups and counting records to summarize those groups. The resulting plots are known as histograms.\nLet’s first look at unaggregated data: a scatter plot showing movie ratings from Rotten Tomatoes versus ratings from IMDB users. We’ll provide data to Altair by passing the movies data URL to the Chart method. (We could also pass the Pandas data frame directly to get the same result.) We can then encode the Rotten Tomatoes and IMDB ratings fields using the x and y channels:\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nTo summarize this data, we can bin a data field to group numeric values into discrete groups. Here we bin along the x-axis by adding bin=True to the x encoding channel. The result is a set of ten bins of equal step size, each corresponding to a span of ten ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=True),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nSetting bin=True uses default binning settings, but we can exercise more control if desired. Let’s instead set the maximum bin count (maxbins) to 20, which has the effect of doubling the number of bins. Now each bin corresponds to a span of five ratings points.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q')\n)\n\n\n\n\n\n\nWith the data binned, let’s now summarize the distribution of Rotten Tomatoes ratings. We will drop the IMDB ratings for now and instead use the y encoding channel to show an aggregate count of records, so that the vertical position of each point indicates the number of movies per Rotten Tomatoes rating bin.\nAs the count aggregate counts the number of total records in each bin regardless of the field values, we do not need to include a field name in the y encoding.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nTo arrive at a standard histogram, let’s change the mark type from circle to bar:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nWe can now examine the distribution of ratings more clearly: we can see fewer movies on the negative end, and a bit more movies on the high end, but a generally uniform distribution overall. Rotten Tomatoes ratings are determined by taking “thumbs up” and “thumbs down” judgments from film critics and calculating the percentage of positive reviews. It appears this approach does a good job of utilizing the full range of rating values.\nSimilarly, we can create a histogram for IMDB ratings by changing the field in the x encoding channel:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('count()')\n)\n\n\n\n\n\n\nIn contrast to the more uniform distribution we saw before, IMDB ratings exhibit a bell-shaped (though negatively skewed) distribution. IMDB ratings are formed by averaging scores (ranging from 1 to 10) provided by the site’s users. We can see that this form of measurement leads to a different shape than the Rotten Tomatoes ratings. We can also see that the mode of the distribution is between 6.5 and 7: people generally enjoy watching movies, potentially explaining the positive bias!\nNow let’s turn back to our scatter plot of Rotten Tomatoes and IMDB ratings. Here’s what happens if we bin both axes of our original plot.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n)\n\n\n\n\n\n\nDetail is lost due to overplotting, with many points drawn directly on top of each other.\nTo form a two-dimensional histogram we can add a count aggregate as before. As both the x and y encoding channels are already taken, we must use a different encoding channel to convey the counts. Here is the result of using circular area by adding a size encoding channel.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Size('count()')\n)\n\n\n\n\n\n\nAlternatively, we can encode counts using the color channel and change the mark type to bar. The result is a two-dimensional histogram in the form of a heatmap.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Y('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20)),\n    alt.Color('count()')\n)\n\n\n\n\n\n\nCompare the size and color-based 2D histograms above. Which encoding do you think should be preferred? Why? In which plot can you more precisely compare the magnitude of individual values? In which plot can you more accurately see the overall density of ratings?",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation</span>"
    ]
  },
  {
    "objectID": "altair_data_transformation.html#aggregation",
    "href": "altair_data_transformation.html#aggregation",
    "title": "3  Data Transformation",
    "section": "3.3 Aggregation",
    "text": "3.3 Aggregation\nCounts are just one type of aggregate. We might also calculate summaries using measures such as the average, median, min, or max. The Altair documentation includes the full set of available aggregation functions.\nLet’s look at some examples!\n\n3.3.1 Averages and Sorting\nDo different genres of films receive consistently different ratings from critics? As a first step towards answering this question, we might examine the average (a.k.a. the arithmetic mean) rating for each genre of movie.\nLet’s visualize genre along the y axis and plot average Rotten Tomatoes ratings along the x axis.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N')\n)\n\n\n\n\n\n\nThere does appear to be some interesting variation, but looking at the data as an alphabetical list is not very helpful for ranking critical reactions to the genres.\nFor a tidier picture, let’s sort the genres in descending order of average rating. To do so, we will add a sort parameter to the y encoding channel, stating that we wish to sort by the average (op, the aggregate operation) Rotten Tomatoes rating (the field) in descending order.\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('average(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='average', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nThe sorted plot suggests that critics think highly of documentaries, musicals, westerns, and dramas, but look down upon romantic comedies and horror films… and who doesn’t love null movies!?\n\n\n3.3.2 Medians and the Inter-Quartile Range\nWhile averages are a common way to summarize data, they can sometimes mislead. For example, very large or very small values (outliers) might skew the average. To be safe, we can compare the genres according to the median ratings as well.\nThe median is a point that splits the data evenly, such that half of the values are less than the median and the other half are greater. The median is less sensitive to outliers and so is referred to as a robust statistic. For example, arbitrarily increasing the largest rating value will not cause the median to change.\nLet’s update our plot to use a median aggregate and sort by those values:\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('median(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\nWe can see that some of the genres with similar averages have swapped places (films of unknown genre, or null, are now rated highest!), but the overall groups have stayed stable. Horror films continue to get little love from professional film critics.\nIt’s a good idea to stay skeptical when viewing aggregate statistics. So far we’ve only looked at point estimates. We have not examined how ratings vary within a genre.\nLet’s visualize the variation among the ratings to add some nuance to our rankings. Here we will encode the inter-quartile range (IQR) for each genre. The IQR is the range in which the middle half of data values reside. A quartile contains 25% of the data values. The inter-quartile range consists of the two middle quartiles, and so contains the middle 50%.\nTo visualize ranges, we can use the x and x2 encoding channels to indicate the starting and ending points. We use the aggregate functions q1 (the lower quartile boundary) and q3 (the upper quartile boundary) to provide the inter-quartile range. (In case you are wondering, q2 would be the median.)\n\nalt.Chart(movies_url).mark_bar().encode(\n    alt.X('q1(Rotten_Tomatoes_Rating):Q'),\n    alt.X2('q3(Rotten_Tomatoes_Rating):Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='median', field='Rotten_Tomatoes_Rating', order='descending')\n    )\n)\n\n\n\n\n\n\n\n\n3.3.3 Time Units\nNow let’s ask a completely different question: do box office returns vary by season?\nTo get an initial answer, let’s plot the median U.S. gross revenue by month.\nTo make this chart, use the timeUnit transform to map release dates to the month of the year. The result is similar to binning, but using meaningful time intervals. Other valid time units include year, quarter, date (numeric day in month), day (day of the week), and hours, as well as compound units such as yearmonth or hoursminutes. See the Altair documentation for a complete list of time units.\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(US_Gross):Q')\n)\n\n\n\n\n\n\nLooking at the resulting plot, median movie sales in the U.S. appear to spike around the summer blockbuster season and the end of year holiday period. Of course, people around the world (not just the U.S.) go out to the movies. Does a similar pattern arise for worldwide gross revenue?\n\nalt.Chart(movies_url).mark_area().encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(Worldwide_Gross):Q')\n)\n\n\n\n\n\n\nYes!",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation</span>"
    ]
  },
  {
    "objectID": "altair_data_transformation.html#advanced-data-transformation",
    "href": "altair_data_transformation.html#advanced-data-transformation",
    "title": "3  Data Transformation",
    "section": "3.4 Advanced Data Transformation",
    "text": "3.4 Advanced Data Transformation\nThe examples above all use transformations (bin, timeUnit, aggregate, sort) that are defined relative to an encoding channel. However, at times you may want to apply a chain of multiple transformations prior to visualization, or use transformations that don’t integrate into encoding definitions. For such cases, Altair and Vega-Lite support data transformations defined separately from encodings. These transformations are applied to the data before any encodings are considered.\nWe could also perform transformations using Pandas directly, and then visualize the result. However, using the built-in transforms allows our visualizations to be published more easily in other contexts; for example, exporting the Vega-Lite JSON to use in a stand-alone web interface. Let’s look at the built-in transforms supported by Altair, such as calculate, filter, aggregate, and window.\n\n3.4.1 Calculate\nThink back to our comparison of U.S. gross and worldwide gross. Doesn’t worldwide revenue include the U.S.? (Indeed it does.) How might we get a better sense of trends outside the U.S.?\nWith the calculate transform we can derive new fields. Here we want to subtract U.S. gross from worldwide gross. The calculate transform takes a Vega expression string to define a formula over a single record. Vega expressions use JavaScript syntax. The datum. prefix accesses a field value on the input record.\n\nalt.Chart(movies).mark_area().transform_calculate(\n    NonUS_Gross='datum.Worldwide_Gross - datum.US_Gross'\n).encode(\n    alt.X('month(Release_Date):T'),\n    alt.Y('median(NonUS_Gross):Q')\n)\n\n\n\n\n\n\nWe can see that seasonal trends hold outside the U.S., but with a more pronounced decline in the non-peak months.\n\n\n3.4.2 Filter\nThe filter transform creates a new table with a subset of the original data, removing rows that fail to meet a provided predicate test. Similar to the calculate transform, filter predicates are expressed using the Vega expression language.\nBelow we add a filter to limit our initial scatter plot of IMDB vs. Rotten Tomatoes ratings to only films in the major genre of “Romantic Comedy”.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('datum.Major_Genre == \"Romantic Comedy\"')\n\n\n\n\n\n\nHow does the plot change if we filter to view other genres? Edit the filter expression to find out.\nNow let’s filter to look at films released before 1970.\n\nalt.Chart(movies_url).mark_circle().encode(\n    alt.X('Rotten_Tomatoes_Rating:Q'),\n    alt.Y('IMDB_Rating:Q')\n).transform_filter('year(datum.Release_Date) &lt; 1970')\n\n\n\n\n\n\nThey seem to score unusually high! Are older films simply better, or is there a selection bias towards more highly-rated older films in this dataset?\n\n\n3.4.3 Aggregate\nWe have already seen aggregate transforms such as count and average in the context of encoding channels. We can also specify aggregates separately, as a pre-processing step for other transforms (as in the window transform examples below). The output of an aggregate transform is a new data table with records that contain both the groupby fields and the computed aggregate measures.\nLet’s recreate our plot of average ratings by genre, but this time using a separate aggregate transform. The output table from the aggregate transform contains 13 rows, one for each genre.\nTo order the y axis we must include a required aggregate operation in our sorting instructions. Here we use the max operator, which works fine because there is only one output record per genre. We could similarly use the min operator and end up with the same plot.\n\nalt.Chart(movies_url).mark_bar().transform_aggregate(\n    groupby=['Major_Genre'],\n    Average_Rating='average(Rotten_Tomatoes_Rating)'\n).encode(\n    alt.X('Average_Rating:Q'),\n    alt.Y('Major_Genre:N', sort=alt.EncodingSortField(\n        op='max', field='Average_Rating', order='descending'\n      )\n    )\n)\n\n\n\n\n\n\n\n\n3.4.4 Window\nThe window transform performs calculations over sorted groups of data records. Window transforms are quite powerful, supporting tasks such as ranking, lead/lag analysis, cumulative totals, and running sums or averages. Values calculated by a window transform are written back to the input data table as new fields. Window operations include the aggregate operations we’ve seen earlier, as well as specialized operations such as rank, row_number, lead, and lag. The Vega-Lite documentation lists all valid window operations.\nOne use case for a window transform is to calculate top-k lists. Let’s plot the top 20 directors in terms of total worldwide gross.\nWe first use a filter transform to remove records for which we don’t know the director. Otherwise, the director null would dominate the list! We then apply an aggregate to sum up the worldwide gross for all films, grouped by director. At this point we could plot a sorted bar chart, but we’d end up with hundreds and hundreds of directors. How can we limit the display to the top 20?\nThe window transform allows us to determine the top directors by calculating their rank order. Within our window transform definition we can sort by gross and use the rank operation to calculate rank scores according to that sort order. We can then add a subsequent filter transform to limit the data to only records with a rank value less than or equal to 20.\n\nalt.Chart(movies_url).mark_bar().transform_filter(\n    'datum.Director != null'\n).transform_aggregate(\n    Gross='sum(Worldwide_Gross)',\n    groupby=['Director']\n).transform_window(\n    Rank='rank()',\n    sort=[alt.SortField('Gross', order='descending')]\n).transform_filter(\n    'datum.Rank &lt; 20'\n).encode(\n    alt.X('Gross:Q'),\n    alt.Y('Director:N', sort=alt.EncodingSortField(\n        op='max', field='Gross', order='descending'\n    ))\n)\n\n\n\n\n\n\nWe can see that Steven Spielberg has been quite successful in his career! However, showing sums might favor directors who have had longer careers, and so have made more movies and thus more money. What happens if we change the choice of aggregate operation? Who is the most successful director in terms of average or median gross per film? Modify the aggregate transform above!\nEarlier in this notebook we looked at histograms, which approximate the probability density function of a set of values. A complementary approach is to look at the cumulative distribution. For example, think of a histogram in which each bin includes not only its own count but also the counts from all previous bins — the result is a running total, with the last bin containing the total number of records. A cumulative chart directly shows us, for a given reference value, how many data values are less than or equal to that reference.\nAs a concrete example, let’s look at the cumulative distribution of films by running time (in minutes). Only a subset of records actually include running time information, so we first filter down to the subset of films for which we have running times. Next, we apply an aggregate to count the number of films per duration (implicitly using “bins” of 1 minute each). We then use a window transform to compute a running total of counts across bins, sorted by increasing running time.\n\nalt.Chart(movies_url).mark_line(interpolate='step-before').transform_filter(\n    'datum.Running_Time_min != null'\n).transform_aggregate(\n    groupby=['Running_Time_min'],\n    Count='count()',\n).transform_window(\n    Cumulative_Sum='sum(Count)',\n    sort=[alt.SortField('Running_Time_min', order='ascending')]\n).encode(\n    alt.X('Running_Time_min:Q', axis=alt.Axis(title='Duration (min)')),\n    alt.Y('Cumulative_Sum:Q', axis=alt.Axis(title='Cumulative Count of Films'))\n)\n\n\n\n\n\n\nLet’s examine the cumulative distribution of film lengths. We can see that films under 110 minutes make up about half of all the films for which we have running times. We see a steady accumulation of films between 90 minutes and 2 hours, after which the distribution begins to taper off. Though rare, the dataset does contain multiple films more than 3 hours long!",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation</span>"
    ]
  },
  {
    "objectID": "altair_data_transformation.html#summary",
    "href": "altair_data_transformation.html#summary",
    "title": "3  Data Transformation",
    "section": "3.5 Summary",
    "text": "3.5 Summary\nWe’ve only scratched the surface of what data transformations can do! For more details, including all the available transformations and their parameters, see the Altair data transformation documentation.\nSometimes you will need to perform significant data transformation to prepare your data prior to using visualization tools. To engage in data wrangling right here in Python, you can use the Pandas library.",
    "crumbs": [
      "Curriculum",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Transformation</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html",
    "href": "altair_debugging.html",
    "title": "4  Altair Debugging Guide",
    "section": "",
    "text": "4.1 Installation\nIn this notebook we show you common debugging techniques that you can use if you run into issues with Altair.\nYou can jump to the following sections:\nIn addition to this notebook, you might find the Frequently Asked Questions and Display Troubleshooting guides helpful.\nThis notebook is part of the data visualization curriculum.\nThese instructions follow the Altair documentation but focus on some specifics for this series of notebooks.\nIn every notebook, we will import the Altair and Vega Datasets packages. If you are running this notebook on Colab, Altair and Vega Datasets should be preinstalled and ready to go. The notebooks in this series are designed for Colab but should also work in Jupyter Lab or the Jupyter Notebook (the notebook requires a bit more setup described below) but additional packages are required.\nIf you are running in Jupyter Lab or Jupyter Notebooks, you have to install the necessary packages by running the following command in your terminal.\nOr if you use Conda\nYou can run command line commands from a code cell by prefixing it with !. For example, to install Altair and Vega Datasets with Pip, you can run the following cell.\n!pip install altair vega_datasets\n\n\nRequirement already satisfied: altair in ./.venv/lib/python3.8/site-packages (4.1.0)\n\nRequirement already satisfied: vega_datasets in ./.venv/lib/python3.8/site-packages (0.9.0)\n\nRequirement already satisfied: jsonschema in ./.venv/lib/python3.8/site-packages (from altair) (3.2.0)\n\nRequirement already satisfied: toolz in ./.venv/lib/python3.8/site-packages (from altair) (0.11.1)\n\nRequirement already satisfied: entrypoints in ./.venv/lib/python3.8/site-packages (from altair) (0.3)\n\nRequirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from altair) (1.20.3)\n\nRequirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from altair) (3.0.0)\n\nRequirement already satisfied: pandas&gt;=0.18 in ./.venv/lib/python3.8/site-packages (from altair) (1.2.4)\n\nRequirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from jsonschema-&gt;altair) (47.1.0)\n\nRequirement already satisfied: six&gt;=1.11.0 in ./.venv/lib/python3.8/site-packages (from jsonschema-&gt;altair) (1.16.0)\n\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in ./.venv/lib/python3.8/site-packages (from jsonschema-&gt;altair) (0.17.3)\n\nRequirement already satisfied: attrs&gt;=17.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema-&gt;altair) (21.2.0)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0.0rc2 in ./.venv/lib/python3.8/site-packages (from jinja2-&gt;altair) (2.0.0)\n\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in ./.venv/lib/python3.8/site-packages (from pandas&gt;=0.18-&gt;altair) (2.8.1)\n\nRequirement already satisfied: pytz&gt;=2017.3 in ./.venv/lib/python3.8/site-packages (from pandas&gt;=0.18-&gt;altair) (2021.1)\n\nWARNING: You are using pip version 20.1.1; however, version 21.1.2 is available.\n\nYou should consider upgrading via the '/Users/jjallaire/quarto/demo/visualization-curriculum/.venv/bin/python3 -m pip install --upgrade pip' command.\nimport altair as alt\nfrom vega_datasets import data",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#installation",
    "href": "altair_debugging.html#installation",
    "title": "4  Altair Debugging Guide",
    "section": "",
    "text": "pip install altair vega_datasets\n\nconda install -c conda-forge altair vega_datasets\n\n\n\n\n4.1.1 Make sure you are Using the Latest Version of Altair\nIf you are running into issues with Altair, first make sure that you are running the latest version. To check the version of Altair that you have installed, run the cell below.\n\nalt.__version__\n\n'4.1.0'\n\n\nTo check what the latest version of altair is, go to this page or run the cell below (requires Python 3).\n\nimport urllib.request, json \nwith urllib.request.urlopen(\"https://pypi.org/pypi/altair/json\") as url:\n    print(json.loads(url.read().decode())['info']['version'])\n\n4.1.0\n\n\nIf you are not running the latest version, you can update it with pip. You can update Altair and Vega Datasets by running this command in your terminal.\npip install -U altair vega_datasets\n\n\n4.1.2 Try Making a Chart\nNow you can create an Altair chart.\n\ncars = data.cars()\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Displacement',\n    color='Origin'\n)\n\n\n\n\n\n\n\n\n4.1.3 Special Setup for the Jupyter Notebook\nIf you are running in Jupyter Lab, Jupyter Notebook, or Colab (and have a working Internet connection) you should be seeing a chart. If you are running in another environment (or offline), you will need to tell Altair to use a different renderer;\nTo activate a different renderer in a notebook cell:\n# to run in nteract, VSCode, or offline in JupyterLab\nalt.renderers.enable('mimebundle')\nTo run offline in Jupyter Notebook you must install an additional dependency, the vega package. Run this command in your terminal:\npip install vega\nThen activate the notebook renderer:\n# to run offline in Jupyter Notebook\nalt.renderers.enable('notebook')\nThese instruction follow the instructions on the Altair website.",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#display-troubleshooting",
    "href": "altair_debugging.html#display-troubleshooting",
    "title": "4  Altair Debugging Guide",
    "section": "4.2 Display Troubleshooting",
    "text": "4.2 Display Troubleshooting\nIf you are having issues with seeing a chart, make sure your setup is correct by following the debugging instruction above. If you are still having issues, follow the instruction about debugging display issues in the Altair documentation.\n\n4.2.1 Non Existent Fields\nA common error is accidentally using a field that does not exit.\n\nimport pandas as pd\n\ndf = pd.DataFrame({'x': [1, 2, 3],\n                     'y': [3, 1, 4]})\n\nalt.Chart(df).mark_point().encode(\n    x='x:Q',\n    y='y:Q',\n    color='color:Q'  # &lt;-- this field does not exist in the data!\n)\n\n\n\n\n\n\nCheck the spelling of your files and print the data source to confirm that the data and fields exist. For instance, here you see that color is not a vaid field.\n\ndf.head()\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n1\n3\n\n\n1\n2\n1\n\n\n2\n3\n4",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#invalid-specifications",
    "href": "altair_debugging.html#invalid-specifications",
    "title": "4  Altair Debugging Guide",
    "section": "4.3 Invalid Specifications",
    "text": "4.3 Invalid Specifications\nAnother common issue is creating an invalid specification and getting an error.\n\n4.3.1 Invalid Properties\nAltair might show an SchemaValidationError or ValueError. Read the error message carefully. Usually it will tell you what is going wrong.\nFor example, if you forget the mark type, you will see this SchemaValidationError.\n\nalt.Chart(cars).encode(\n    y='Horsepower'\n)\n\n\n---------------------------------------------------------------------------\nSchemaValidationError                     Traceback (most recent call last)\n~/quarto/demo/visualization-curriculum/.venv/lib/python3.8/site-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs)\n    380         if dct is None:\n    381             kwargs[\"validate\"] = \"deep\"\n--&gt; 382             dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs)\n    383 \n    384         # TODO: following entries are added after validation. Should they be validated?\n\n~/quarto/demo/visualization-curriculum/.venv/lib/python3.8/site-packages/altair/utils/schemapi.py in to_dict(self, validate, ignore, context)\n    337                 self.validate(result)\n    338             except jsonschema.ValidationError as err:\n--&gt; 339                 raise SchemaValidationError(self, err)\n    340         return result\n    341 \n\nSchemaValidationError: Invalid specification\n\n        altair.vegalite.v4.api.Chart, validating 'required'\n\n        'mark' is a required property\n        \n\n\n\nalt.Chart(...)\n\n\nOr if you use a non-existent channel, you get a ValueError.\n\nalt.Chart(cars)).mark_point().encode(\n    z='Horsepower'\n)\n\n\n  File \"&lt;ipython-input-9-fc84db126677&gt;\", line 1\n    alt.Chart(cars)).mark_point().encode(\n                   ^\nSyntaxError: unmatched ')'",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#properties-are-being-ignored",
    "href": "altair_debugging.html#properties-are-being-ignored",
    "title": "4  Altair Debugging Guide",
    "section": "4.4 Properties are Being Ignored",
    "text": "4.4 Properties are Being Ignored\nAltair might ignore a property that you specified. In the chart below, we are using a text channel, which is only compatible with mark_text. You do not see an error or a warning about this in the notebook. However, the underlying Vega-Lite library will show a warning in the browser console. Press Alt+Cmd+I on Mac or Alt+Ctrl+I on Windows and Linux to open the developer tools and click on the Console tab. When you run the example in the cell below, you will see a the following warning.\nWARN text dropped as it is incompatible with \"bar\".\n\nalt.Chart(cars).mark_bar().encode(\n    y='mean(Horsepower)',\n    text='mean(Acceleration)'\n)\n\n\n\n\n\n\nIf you find yourself debugging issues related to Vega-Lite, you can open the chart in the Vega Editor either by clicking on the “Open in Vega Editor” link at the bottom of the chart or in the action menu (click to open) at the top right of a chart. The Vega Editor provides additional debugging but you will be writing Vega-Lite JSON instead of Altair in Python.\nNote: The Vega Editor may be using a newer version of Vega-Lite and so the behavior may vary.",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#asking-for-help",
    "href": "altair_debugging.html#asking-for-help",
    "title": "4  Altair Debugging Guide",
    "section": "4.5 Asking for Help",
    "text": "4.5 Asking for Help\nIf you find a problem with Altair and get stuck, you can ask a question on Stack Overflow. Ask your question with the altair and vega-lite tags. You can find a list of questions people have asked before here.",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  },
  {
    "objectID": "altair_debugging.html#reporting-issues",
    "href": "altair_debugging.html#reporting-issues",
    "title": "4  Altair Debugging Guide",
    "section": "4.6 Reporting Issues",
    "text": "4.6 Reporting Issues\nIf you find a problem with Altair and believe it is a bug, please create an issue in the Altair GitHub repo with a description of your problem. If you believe the issue is related to the underlying Vega-Lite library, please create an issue in the Vega-Lite GitHub repo.",
    "crumbs": [
      "Support",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Altair Debugging Guide</span>"
    ]
  }
]